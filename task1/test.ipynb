{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "## Constant for Cost function\n",
    "THRESHOLD = 0.5\n",
    "W1 = 1\n",
    "W2 = 20\n",
    "W3 = 100\n",
    "W4 = 0.04\n",
    "\n",
    "\n",
    "def cost_function(true, predicted):\n",
    "    \"\"\"\n",
    "        true: true values in 1D numpy array\n",
    "        predicted: predicted values in 1D numpy array\n",
    "\n",
    "        return: float\n",
    "    \"\"\"\n",
    "    cost = (true - predicted)**2\n",
    "\n",
    "    # true above threshold (case 1)\n",
    "    mask = true > THRESHOLD\n",
    "    mask_w1 = np.logical_and(predicted>=true,mask)\n",
    "    mask_w2 = np.logical_and(np.logical_and(predicted<true,predicted >=THRESHOLD),mask)\n",
    "    mask_w3 = np.logical_and(predicted<THRESHOLD,mask)\n",
    "\n",
    "    cost[mask_w1] = cost[mask_w1]*W1\n",
    "    cost[mask_w2] = cost[mask_w2]*W2\n",
    "    cost[mask_w3] = cost[mask_w3]*W3\n",
    "\n",
    "    # true value below threshold (case 2)\n",
    "    mask = true <= THRESHOLD\n",
    "    mask_w1 = np.logical_and(predicted>true,mask)\n",
    "    mask_w2 = np.logical_and(predicted<=true,mask)\n",
    "\n",
    "    cost[mask_w1] = cost[mask_w1]*W1\n",
    "    cost[mask_w2] = cost[mask_w2]*W2\n",
    "\n",
    "    reward = W4*np.logical_and(predicted < THRESHOLD,true<THRESHOLD)\n",
    "    if reward is None:\n",
    "        reward = 0\n",
    "    return np.mean(cost) - np.mean(reward)\n",
    "\n",
    "\"\"\"\n",
    "Fill in the methods of the Model. Please do not change the given methods for the checker script to work.\n",
    "You can add new methods, and make changes. The checker script performs:\n",
    "\n",
    "\n",
    "    M = Model()\n",
    "    M.fit_model(train_x,train_y)\n",
    "    prediction = M.predict(test_x)\n",
    "\n",
    "It uses predictions to compare to the ground truth using the cost_function above.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# The code is copyed from the first tutorial of Gpytorch\n",
    "# link: https://docs.gpytorch.ai/en/v1.1.1/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "       \n",
    "\n",
    "    def predict(self, test_x):\n",
    "        test_x = torch.Tensor(test_x)\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            observed_pred = self.likelihood(self.model(test_x))\n",
    "        \n",
    "        return observed_pred.mean.numpy()\n",
    "\n",
    "    def fit_model(self, train_x, train_y):\n",
    "        \n",
    "        training_iter = 10\n",
    "        \n",
    "        train_x = torch.Tensor(train_x)\n",
    "        train_y = torch.Tensor(train_y)\n",
    "        \n",
    "        self.model = ExactGPModel(train_x, train_y, self.likelihood)\n",
    "        \n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        # Use the adam optimizer\n",
    "        optimizer = torch.optim.Adam([\n",
    "        {'params': self.model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "        ], lr=0.1)\n",
    "\n",
    "        # \"Loss\" for GPs - the marginal log likelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "        \n",
    "        for i in range(training_iter):\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            output = self.model(train_x)\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                self.model.covar_module.base_kernel.lengthscale.item(),\n",
    "                self.model.likelihood.noise.item()\n",
    "            ))\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x_name = \"train_x.csv\"\n",
    "train_y_name = \"train_y.csv\"\n",
    "\n",
    "train_x = np.loadtxt(train_x_name, delimiter=',')\n",
    "train_y = np.loadtxt(train_y_name, delimiter=',')\n",
    "\n",
    "# load the test dateset\n",
    "test_x_name = \"test_x.csv\"\n",
    "test_x = np.loadtxt(test_x_name, delimiter=',')\n",
    "\n",
    "M = Model()\n",
    "M.fit_model(train_x, train_y)\n",
    "prediction = M.predict(test_x)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_name = \"train_x.csv\"\n",
    "train_y_name = \"train_y.csv\"\n",
    "\n",
    "train_x = np.loadtxt(train_x_name, delimiter=',')\n",
    "train_y = np.loadtxt(train_y_name, delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "# load the test dateset\n",
    "test_x_name = \"test_x.csv\"\n",
    "test_x = np.loadtxt(test_x_name, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(train_x[:,0], train_x[:,1], s=1, c=train_y)\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(test_x[:,0], test_x[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nystroem=Nystroem(gamma=1)\n",
    "x_transformed = nystroem.fit_transform(train_x[:,1].reshape(-1, 1))\n",
    "\n",
    "rlf = GaussianProcessRegressor()\n",
    "rlf.fit(x_transformed, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.mgrid[-0.5:1:0.025, -1:1:0.05].reshape(2,-1).T\n",
    "xy_trans = nystroem.transform(xy[:,1].reshape(-1, 1))\n",
    "y_linespace = rlf.predict(xy_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(xy[:,0], xy[:,1], s=10, c=y_linespace)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(xy[:,0], xy[:,1], s=10, c=y_linespace)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, std = rlf.predict(transformed_x, return_std=True)\n",
    "x, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, std_dev = rlf.predict(val_x, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_pred = np.ones(val_y.shape)\n",
    "prop_pred[(pred <0.5) & (std_dev > 0)] = norm(pred[(pred <0.5) & (std_dev > 0)], std_dev[(pred <0.5) & (std_dev > 0)]).cdf(0.5)\n",
    "prop_pred[prop_pred < 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[prop_pred<0.99] = 0.5\n",
    "val_x[(pred < 0.5) & (val_y > 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-726ceadec41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "cost_function(val_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_trans = nystroem.transform(val_x)\n",
    "cost_function(val_y, rlf.predict(val_x_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# load the test dateset\n",
    "test_x_name = \"test_x.csv\"\n",
    "test_x = np.loadtxt(test_x_name, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_name = \"train_x.csv\"\n",
    "train_y_name = \"train_y.csv\"\n",
    "\n",
    "train_x = np.loadtxt(train_x_name, delimiter=',')\n",
    "train_y = np.loadtxt(train_y_name, delimiter=',')\n",
    "index = train_x[:, 0]>-0.4\n",
    "X_train, val_x, y_train, val_y = train_test_split(\n",
    "    train_x[index], \n",
    "    train_y[index], \n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "train_x = np.concatenate((X_train, train_x[~index]))\n",
    "train_y = np.append(y_train, train_y[~index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "val_x = torch.Tensor(val_x)\n",
    "val_y = torch.Tensor(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.741   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 0.705   lengthscale: 0.744   noise: 0.644\n",
      "Iter 3/50 - Loss: 0.667   lengthscale: 0.798   noise: 0.598\n",
      "Iter 4/50 - Loss: 0.630   lengthscale: 0.853   noise: 0.554\n",
      "Iter 5/50 - Loss: 0.591   lengthscale: 0.908   noise: 0.513\n",
      "Iter 6/50 - Loss: 0.552   lengthscale: 0.959   noise: 0.474\n",
      "Iter 7/50 - Loss: 0.512   lengthscale: 0.998   noise: 0.437\n",
      "Iter 8/50 - Loss: 0.471   lengthscale: 1.019   noise: 0.403\n",
      "Iter 9/50 - Loss: 0.430   lengthscale: 1.024   noise: 0.370\n",
      "Iter 10/50 - Loss: 0.388   lengthscale: 1.016   noise: 0.340\n",
      "Iter 11/50 - Loss: 0.345   lengthscale: 1.001   noise: 0.312\n",
      "Iter 12/50 - Loss: 0.302   lengthscale: 0.982   noise: 0.286\n",
      "Iter 13/50 - Loss: 0.258   lengthscale: 0.957   noise: 0.261\n",
      "Iter 14/50 - Loss: 0.214   lengthscale: 0.929   noise: 0.239\n",
      "Iter 15/50 - Loss: 0.169   lengthscale: 0.901   noise: 0.218\n",
      "Iter 16/50 - Loss: 0.124   lengthscale: 0.872   noise: 0.199\n",
      "Iter 17/50 - Loss: 0.078   lengthscale: 0.842   noise: 0.181\n",
      "Iter 18/50 - Loss: 0.032   lengthscale: 0.813   noise: 0.165\n",
      "Iter 19/50 - Loss: -0.014   lengthscale: 0.782   noise: 0.150\n",
      "Iter 20/50 - Loss: -0.061   lengthscale: 0.749   noise: 0.136\n",
      "Iter 21/50 - Loss: -0.108   lengthscale: 0.710   noise: 0.123\n",
      "Iter 22/50 - Loss: -0.155   lengthscale: 0.668   noise: 0.112\n",
      "Iter 23/50 - Loss: -0.203   lengthscale: 0.625   noise: 0.101\n",
      "Iter 24/50 - Loss: -0.251   lengthscale: 0.582   noise: 0.092\n",
      "Iter 25/50 - Loss: -0.299   lengthscale: 0.538   noise: 0.083\n",
      "Iter 26/50 - Loss: -0.348   lengthscale: 0.495   noise: 0.075\n",
      "Iter 27/50 - Loss: -0.396   lengthscale: 0.454   noise: 0.068\n",
      "Iter 28/50 - Loss: -0.443   lengthscale: 0.417   noise: 0.062\n",
      "Iter 29/50 - Loss: -0.492   lengthscale: 0.384   noise: 0.056\n",
      "Iter 30/50 - Loss: -0.539   lengthscale: 0.353   noise: 0.050\n",
      "Iter 31/50 - Loss: -0.587   lengthscale: 0.324   noise: 0.045\n",
      "Iter 32/50 - Loss: -0.634   lengthscale: 0.294   noise: 0.041\n",
      "Iter 33/50 - Loss: -0.683   lengthscale: 0.266   noise: 0.037\n",
      "Iter 34/50 - Loss: -0.729   lengthscale: 0.240   noise: 0.033\n",
      "Iter 35/50 - Loss: -0.778   lengthscale: 0.215   noise: 0.030\n",
      "Iter 36/50 - Loss: -0.823   lengthscale: 0.192   noise: 0.027\n",
      "Iter 37/50 - Loss: -0.866   lengthscale: 0.172   noise: 0.025\n",
      "Iter 38/50 - Loss: -0.904   lengthscale: 0.153   noise: 0.022\n",
      "Iter 39/50 - Loss: -0.943   lengthscale: 0.137   noise: 0.020\n",
      "Iter 40/50 - Loss: -0.981   lengthscale: 0.121   noise: 0.018\n",
      "Iter 41/50 - Loss: -1.012   lengthscale: 0.107   noise: 0.016\n",
      "Iter 42/50 - Loss: -1.051   lengthscale: 0.100   noise: 0.015\n",
      "Iter 43/50 - Loss: -1.090   lengthscale: 0.099   noise: 0.013\n",
      "Iter 44/50 - Loss: -1.121   lengthscale: 0.101   noise: 0.012\n",
      "Iter 45/50 - Loss: -1.166   lengthscale: 0.106   noise: 0.011\n",
      "Iter 46/50 - Loss: -1.204   lengthscale: 0.112   noise: 0.010\n",
      "Iter 47/50 - Loss: -1.242   lengthscale: 0.114   noise: 0.009\n",
      "Iter 48/50 - Loss: -1.277   lengthscale: 0.114   noise: 0.008\n",
      "Iter 49/50 - Loss: -1.304   lengthscale: 0.110   noise: 0.007\n",
      "Iter 50/50 - Loss: -1.323   lengthscale: 0.105   noise: 0.007\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "training_iter = 50\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_name = \"test_x.csv\"\n",
    "test_x = np.loadtxt(test_x_name, delimiter=',')\n",
    "test_x = torch.Tensor(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted = observed_pred.mean.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00101813834802858"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_function(true, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "var = observed_pred.variance.detach()\n",
    "pred_norm = norm(predicted, var.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35948405 0.41861215 0.3176902  0.33265367 0.37785214 0.33506316\n",
      " 0.3619866  0.33708006 0.45552596 0.43954518 0.32057878 0.3208769\n",
      " 0.44594634 0.32575798 0.34865913 0.43650407 0.44257608 0.4456481\n",
      " 0.4095768  0.45005193 0.31999668 0.3359889  0.39083293 0.33462504\n",
      " 0.33674854 0.3663136  0.40022853 0.3468207  0.37068143 0.33303675\n",
      " 0.4559605  0.32236627 0.3880891  0.39630863 0.3701878  0.35681632\n",
      " 0.34067288 0.42975214 0.37976256 0.34389517 0.3332869  0.34545606\n",
      " 0.350161   0.3218861  0.40188524 0.35644448 0.3931515  0.3357171\n",
      " 0.32446507 0.40370932 0.33630204 0.39284942 0.34827277 0.3367366\n",
      " 0.38755718 0.4638171  0.41204992]\n"
     ]
    }
   ],
   "source": [
    "pred_new = predicted.copy()\n",
    "prop = 1-pred_norm.cdf([0.5]*len(predicted))\n",
    "pred_new[(predicted<0.5) & (prop > 0.3)] = 0.5\n",
    "print(predicted[(predicted<0.5)& (prop > 0.3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.00101813834802858, -0.00101813834802858)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-cee60c0907d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'scatter'"
     ]
    }
   ],
   "source": [
    "plt.scatter(test_x[:,0], test_x[:,1], s=50, c=observed_pred.mean)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(test_x[:,0], test_x[:,1], s=50, c=observed_pred.variance.detach().numpy())\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PAI",
   "language": "python",
   "name": "pai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
